{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 9: Methods using KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Live exercise\n",
    "Work in pairs and use Internet to find anything related to\n",
    "* Sequential Quadratic Programming (SQP) and\n",
    "* Augmented Lagrangian method\n",
    "\n",
    "Time: 10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Quadratic Programming (SQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea is to generate a sequence of quadratic optimization problems whose solutions approach the solution of the original problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider problem\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t.Â }h_k(x) = 0\\text{ for all }k=1,\\ldots,K,\n",
    "$$\n",
    "where the the objective function and the equality constraints are twice differentiable. \n",
    "\n",
    "Inequality constraints can be handled e.g. by using active set methods. See e.g. https://optimization.mccormick.northwestern.edu/index.php/Sequential_quadratic_programming\n",
    "\n",
    "Note that constraints can be nonlinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because we know that the optimal solution of this problem satisfies the KKT conditions, we know that\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "\\nabla_xL(x,\\lambda)=\\nabla f(x) + \\lambda\\nabla h(x) = 0\\\\\n",
    "h(x) = 0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "Let us assume that we have a current esimation for the solution of the equality constraints $(x^k,\\lambda^k)$, then according to the Newton's method for root finding (see e.g., https://en.wikipedia.org/wiki/Newton%27s_method ), we have another solution $(x^k,\\lambda^k)^T+(p,v)^T$ of the problem by solving system of equations\n",
    "$$\n",
    "\\nabla_{x}S(x^k,\\lambda^k)\\left[\\begin{align}p^T\\\\v^T\\end{align}\\right] = -S(x^k,\\lambda^k),\n",
    "$$\n",
    "where $S(x^k,\\lambda^k)=\\left[\n",
    "\\begin{array}{c}\n",
    "\\nabla_{x}L(x^k,\\lambda^k)\\\\\n",
    "h(x^k)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$. This can be written as\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\nabla^2_{xx}L(x^k,\\lambda^k)&\\nabla h(x^k)\\\\\n",
    "\\nabla h(x^k)^T & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\\begin{array}{c}p^T\\\\v^T\\end{array}\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "-\\nabla_x L(x^k,\\lambda^k)\\\\\n",
    "-h(x^k)^T\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "However, the above is just the solution of the quadratic problem with equality constraints\n",
    "$$\n",
    "\\min \\frac12 p^T\\nabla^2_{xx}L(x^k,\\lambda^k)p+\\nabla_xL(x^k,\\lambda^k)^Tp\\\\\n",
    "\\text{s.t. }h_j(x^k) + \\nabla h_j(x^k)^Tp = 0. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuitive interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are approximating the Lagrange function quadratically around the current solution and the constraints are approximated linearly. SQP methods are also referred to *projected Lagrangian methods*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Another view point to building the approximation**: Assume that we have a current solution candidate $(x^k,\\lambda^k)$. Using Taylor's series for the constraints ($d = x^*-x^k$) and including only the first order term we get\n",
    "\n",
    "$h_i(x^k+d)\\approx h_i(x^k) + \\nabla h_i(x^k)^Td$\n",
    "\n",
    "Since, $h_i(x^*)=0$ for all $i$ we have\n",
    "\n",
    "$\\nabla h(x^k)d = -h(x^k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For approximating the Lagrangian function, we use up to second order terms and get\n",
    "\n",
    "$L(x^k+d,\\lambda^*) \\approx L(x^k,\\lambda^*) + d^T\\nabla_x L(x^k,\\lambda^*) + \\frac{1}{2}d^T\\nabla_{xx}^2L(x^k,\\lambda^*)d$\n",
    "\n",
    "When combining both approximations, we get a quadratic subproblem\n",
    "\n",
    "$$\n",
    "\\underset{d}{\\min}d^T\\nabla_x L(x^k,\\lambda^k) + \\frac{1}{2}d^T \\nabla_{xx}^2L(x^k,\\lambda^k)d\\\\\n",
    "\\text{s.t. } \\nabla h(x^k)d = -h(x^k)\n",
    "$$\n",
    "\n",
    "It can be shown (under some assumptions) that solutions of the quadratic subproblems approach $x^*$ annd Lagrange multipliers approach $\\lambda^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can either use the exact Hessian of the Lagrange function (requires second derivatives) or some approximation of it (compare Newton's method vs. quasi-Newton ideas). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an optimization problem, where\n",
    "* $f(x) = \\|x\\|^2 = \\sum_{i=1}^n x_i^2$\n",
    "* $h(x) = \\sum_{i=1}^nx_i-n$\n",
    "\n",
    "What is $x^*$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_constrained(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-len(x)]\n",
    "#    return sum([i**2 for i in x]),[],[sum(x)-len(x),x[0]**2+x[1]-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_constrained([1,0,1]))\n",
    "print(f_constrained([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "\n",
    "\n",
    "#if k=0, returns the gradient of lagrangian, if k=1, returns the hessian\n",
    "def diff_L(f,x,l,k):\n",
    "    #Define the lagrangian for given m and f\n",
    "    L = lambda x_: f(x_)[0] + (np.matrix(f(x_)[2])*np.matrix(l).transpose())[0,0]\n",
    "    return ad.gh(L)[k](x)\n",
    "#Returns the gradients of the equality constraints\n",
    "def grad_h(f,x):\n",
    "    return  [ad.gh(lambda y:\n",
    "                   f(y)[2][i])[0](x) for i in range(len(f(x)[2]))] \n",
    "\n",
    "#Solves the quadratic problem inside the SQP method\n",
    "def solve_QP(f,x,l):\n",
    "    left_side_first_row = np.concatenate((\\\n",
    "    np.matrix(diff_L(f,x,l,1)),\\\n",
    "    np.matrix(grad_h(f,x)).transpose()),axis=1)\n",
    "    left_side_second_row = np.concatenate((\\\n",
    "    np.matrix(grad_h(f,x)),\\\n",
    "    np.matrix(np.zeros((len(f(x)[2]),len(f(x)[2]))))),axis=1)\n",
    "    right_hand_side = np.concatenate((\\\n",
    "    -1*np.matrix(diff_L(f,x,l,0)).transpose(),\n",
    "    -np.matrix(f(x)[2]).transpose()),axis = 0)\n",
    "    left_hand_side = np.concatenate((\\\n",
    "                                    left_side_first_row,\\\n",
    "                                    left_side_second_row),axis = 0)\n",
    "    temp = np.linalg.solve(left_hand_side,right_hand_side)\n",
    "    return temp[:len(x)],temp[len(x):]\n",
    "    \n",
    "    \n",
    "\n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    l = np.ones(len(f(x)[2]))\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x)[0]\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        print(x)\n",
    "        f_old = f_new\n",
    "        (p,v) = solve_QP(f,x,l)\n",
    "        x = x+np.array(p.transpose())[0]\n",
    "        l = l+v\n",
    "        f_new = f(x)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [2000.0,1000.0,3000.0,5000.0,6000.0]\n",
    "SQP(f_constrained,x0,0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lagrangian methods -- \"The original method of multipliers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again consider problem\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }h_k(x) = 0\\text{ for all }k=1,\\ldots,K,\n",
    "$$\n",
    "where the objective function and the equality constraints are twice differentiable. Inequality constaints can be handled e.g. by first converting them into equality constraints which increases the number of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that if $x^*$ is optimal solution, then $\\nabla_x L(x^*,\\lambda^*) = \\nabla f(x^*) + \\sum_{k=1}^K\\lambda^*_k \\nabla h_k(x^*) = 0$ \n",
    "(<i>necessary condition</i>). \n",
    "\n",
    "<b>However</b>, if we only know that for $x^*$ it holds that $\\nabla_x L(x^*,\\lambda^*) = 0$, we can't be sure that $x^*$ is a local minimizer. (Why???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the Hessian $\\nabla^2_{xx}L(x^*,\\lambda^*)$ may be indefinitive, it is not sufficient to just minimize the Langrangian function $L(x,\\lambda)$ in order to minimize $f(x)$ with respect to the equality constraints $h_k(x)=0$.\n",
    "\n",
    "<i>Solution: Improve Lagrangian function</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define augmented Lagrangian function\n",
    "$$\n",
    "L_c(x,\\lambda) = f(x)+\\lambda h(x)+\\frac12c\\|h(x)\\|^2.\n",
    "$$\n",
    "Above $c\\in \\mathbb R$ is a penalty parameter and $\\lambda \\in \\mathbb R^K$ is a multiplier.\n",
    "\n",
    "Now, we have\n",
    "$$\n",
    "\\nabla^2_{xx}L_c(x^*,\\lambda^*) = \\nabla^2_{xx}L(x^*,\\lambda^*) + c\\nabla h(x^*)^T\\nabla h(x^*)\n",
    "$$\n",
    "and it can be shown that for $c>\\hat{c}$ the Hessian of the augmented Lagrangian is positive definite (<i>sufficient condition for optimality</i>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us consider sequence of optimization problems\n",
    "$$\n",
    "\\min_{x\\in\\mathbb R^n} f(x)+\\lambda_k h(x)+\\frac{1}{2}c_k\\|h(x)\\|^2,\n",
    "$$\n",
    "where $c_{k+1}>c_k$ for $k=1,2,\\ldots$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if $\\lambda_k=0$ for all $k=1,2,\\ldots$, then we have a penalty function method, which solves the problem when $c_k\\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be shown, that if we set $\\lambda_0$ randomly and keep on updating\n",
    "$\\lambda_{k+1} = \\lambda_k+c_kh(x_k)$, then we can show that there exists $C>0$ such that of $c_k>C$, then the optimal solution of the augmented Langrangian solves the original problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have optimization problem\n",
    "$$\n",
    "\\min x_1^2+x_2^2\\\\\n",
    "\\text{s.t. }x_1+x_2-1=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the minimization of the augmented Lagrangian becomes\n",
    "$$\n",
    "\\min_{x\\in\\mathbb R^n} x_1^2+x_2^2+\\lambda_k(x_1+x_2-1)+\\frac12c_k(x_1+x_2-1)^2.\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_constrained2(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_langrangian(f,x,la,c):\n",
    "    second_term = float(numpy.matrix(la)*numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5*c*numpy.linalg.norm(f(x)[2])**2\n",
    "    return f(x)[0]+second_term+third_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def augmented_langrangian_method(f,start,la0,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    la = la0\n",
    "    c = c0\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001:\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,la,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        la = float(la+numpy.matrix(c)*numpy.matrix(f(res.x)[2]).transpose())\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c\n",
    "    return x_new,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def penalty_function_method(f,start,c0):\n",
    "    x_old = float('inf')\n",
    "    x_new = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x_new)[0]\n",
    "    c = c0\n",
    "    while abs(f_old-f_new)>0.00001:\n",
    "#    while numpy.linalg.norm(f(x_new)[2])>0.00001:\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,0,c),x_new)\n",
    "        x_old = x_new\n",
    "        f_old = f_new\n",
    "        x_new = res.x\n",
    "        f_new = f(x_new)[0]\n",
    "        c = 2*c\n",
    "    return x_new,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 =[10,-5]\n",
    "#x0 =[1/3,2/3]\n",
    "augmented_langrangian_method(f_constrained2,x0,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_function_method(f_constrained2,x0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQP(f_constrained2,x0,0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is going on in here?\n",
    "\n",
    "The above is a simplified representation of the augmented Lagrangian method. For example, one can use exact second derivatives for calculating $\\nabla^2_{xx}L(x^*,\\mu^*)$ to obtain better convergence but, also, one can approximate it by utilizing ideas from quasi-Newton methods in order to not requiring second derivatives. Efficient implementation of this (and other methods) for practical problems is not completely trivial, unfortunately. If you want to read details, please see e.g., http://www.mit.edu/~dimitrib/Constrained-Opt.pdf."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
