{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:95% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Steepest descent and Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us define the same function as on the previous lesson for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient-based optimization\n",
    "\n",
    "If gradients are available, they should always be used --> improve convergence\n",
    "\n",
    "If analytical formulas of the gradients are not available, they can be numerically approximated by using finite differences ($‚Ñé>0$)\n",
    "* forward difference: $\\frac{f(x+h)-f(x)}{h}=\\nabla f(x)+O(h)$\n",
    "* central difference: $\\frac{f(x+h)-f(x-h)}{2h}=\\nabla f(x)+O(h^2)$\n",
    "* central difference is more accurate but requires twice as many function evaluations (in $ùë•+‚Ñé$ and $ùë•‚àí‚Ñé$)\n",
    "\n",
    "Gradient-based methods usually converge to a local minimum which is nearest to the starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic differentiation in Python\n",
    "\n",
    "An alternative way is to use automatic differentiation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Automatic_differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import automatic differentiation package for Python\n",
    "\n",
    "Needs to be installed typing\n",
    "```\n",
    "pip install ad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can ask for gradient and hessian using the <pre>ad.gh</pre> function. Let us do that for the function <it>f</it>  that we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import ad\n",
    "grad_f, hess_f = ad.gh(f_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"At the point (1,2) gradient is \", grad_f([1,2]), \" and hessian is \",hess_f([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let us visualize the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from pylab import meshgrid\n",
    "def visualize_gradient(f,point,x_lim,y_lim):\n",
    "    grad_point = np.array(ad.gh(f)[0](point))\n",
    "    grad_point = grad_point/np.linalg.norm(grad_point)\n",
    "    X,Y,Z = point[0],point[1],f(point)\n",
    "    U,V,W = grad_point[0],grad_point[1],0\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    x = np.arange(x_lim[0],x_lim[1],0.1)\n",
    "    y = np.arange(y_lim[0],y_lim[1],0.1)\n",
    "    X2,Y2 = meshgrid(x, y) # grid of point\n",
    "    Z2 = [f([x,y]) for (x,y) in zip (X2,Y2)] # evaluation of the function on the grid\n",
    "    Z2 = np.asarray(Z2)\n",
    "    surf = ax.plot_surface(X2, Y2, Z2,alpha=0.5)\n",
    "    ax.quiver(X,Y,Z,U,V,W,color='red',linewidth=2.5)\n",
    "    return plt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gradient(f_simple,[1,-2],[0,10],[-10,0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With the lambda function we can easily visualize gradients of various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "visualize_gradient(lambda x:3*x[0]+x[1],[1,1],[0,2],[0,2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Base algorithm for the steepest descent and Newton's algorithms\n",
    "**Input:** function $f$ to be optimized, starting point $x_0$, step length rule $alpha$, stopping rule $stop$  \n",
    "**Output:** A solution $x^*$ that is close to a locally optimal solution\n",
    "```\n",
    "set f_old as a big number and f_new as f(x0)\n",
    "while a stopping criterion has not been met:\n",
    "    f_old = f_new\n",
    "    determine search direction d_h according to the method\n",
    "    determine the step length alpha\n",
    "    set x = x + alpha *d_h\n",
    "    f_new = f(x)\n",
    "return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to determine search direction distinguishes steepest descent algorithm and the Newton algorithm. Different stopping rules and step sizes can be mixed and matched with both algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Steepest Descent algorithm for unconstrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also known as *gradient descent* or *gradient method*\n",
    "\n",
    "In the steepest descent algorithm, the search direction is determined by the negative of the gradient $-\\nabla f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use a simple stopping rule, where we stop when the change is not bigger than precision and we have a fixed step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def steepest_descent(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    d = float('Inf')\n",
    "    while abs(f_old-f_new)>precision:\n",
    "    #while np.linalg.norm(d)>precision:\n",
    "        f_old = f_new\n",
    "        d = -np.array(ad.gh(f)[0](x))\n",
    "        x = x+d*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Solve the problem using the Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple,start,.2,0.01)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the steps of solving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_2d_steps(steps,start):\n",
    "    myvec = np.array([start]+steps).transpose()\n",
    "    plt.plot(myvec[0,],myvec[1,],'ro')\n",
    "    for label,x,y in zip([str(i) for i in range(len(steps)+1)],myvec[0,],myvec[1,]):\n",
    "        plt.annotate(label,xy = (x, y))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_simple2(x):\n",
    "    return (x[0] - 2.0)**4 + (x[0] - 2.0*x[1])**2\n",
    "# Optimal solution is (2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [0.0,3.0]\n",
    "(x_value,f_value,steps) = steepest_descent(f_simple2,start,.02,0.0001)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))\n",
    "#print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare steepest descent with optimized step size:\n",
    "![](images/steepest_descent.png)\n",
    "*From Miettinen: Nonlinear optimization, 2007 (in Finnish)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Based on setting the research direction as $-[Hf(x)]^{-1}\\nabla f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one-dimensional case, it is easy to see that since\n",
    "$$f(x+\\Delta x)\\approx f(x)+f'(x)\\Delta x+\\frac12f''(x)\\Delta x^2$$\n",
    "with the Taylor series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to find $x$ such that $f(x)$ is at minimum and, thus, we seek to solve the equation that sets the derivative of this expression with respect to $\\Delta x$ equal to zero:\n",
    "\n",
    "$$ 0 = \\frac{d}{d\\Delta x} \\left(f(x_n)+f'(x_n)\\Delta x+\\frac 1 2 f''(x_n) \\Delta x^2\\right) = f'(x_n)+f'' (x_n) \\Delta x.$$\n",
    "\n",
    "The solution of the above equation is $\\Delta x=-f'(x_n)/f''(x_n)$. Thus, the best approximation of $x_{n+1}$ as the minimum is $x_n-f''(x_n)^{-1}f'(x_n)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def newton(f,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        H_inv = np.linalg.inv(np.matrix(ad.gh(f)[1](x)))\n",
    "        d = (-H_inv*(np.matrix(ad.gh(f)[0](x)).transpose())).transpose()\n",
    "        #Change the type from np.matrix to np.array so that we can use it in our function\n",
    "        x = np.array(x+d*step)[0]\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [2.0,-10.0]\n",
    "(x_value,f_value,steps) = newton(f_simple,start,0.2,0.0001)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [0.0,3.0]\n",
    "(x_value,f_value,steps) = newton(f_simple2,start,1.2,0.0001)\n",
    "print(\"Optimal solution is \",x_value)\n",
    "print(len(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_steps(steps,start).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare Newton's method with optimized step size:\n",
    "![](images/newton.png)\n",
    "*From Miettinen: Nonlinear optimization, 2007 (in Finnish)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Steepest descent\n",
    "* Descent direction: yes\n",
    "* Global convergence: yes\n",
    "* Local convergence: zig-zag near optimum\n",
    "* Computational bottle neck: step length\n",
    "* Memory consumption: $ùëÇ(ùëõ)$\n",
    "\n",
    "Newton‚Äôs method\n",
    "* Descent direction: only if $ùêªf(ùë•)^{‚àí1}$ positive definite\n",
    "* Global convergence: no\n",
    "* Local convergence: yes, good if ùëì quadratic\n",
    "* Computational bottle neck: $ùêªf(ùë• )^{‚àí1}$ --> can be relaxed by using approximations of $Hf(x)^{-1}$ --> quasi-Newton\n",
    "* Memory consumption: $ùëÇ(ùëõ^2)$ --> can be reduced e.g. by using conjugate gradient methods"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
