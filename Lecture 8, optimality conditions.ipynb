{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 8, Optimality conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are still studying the full problem\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to identify which points are optimal, we want to define similar conditions as there are for unconstrained problems through the gradient:\n",
    "\n",
    ">If $x$ is a  local optimum to function $f$, then $\\nabla f(x)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KKT conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Theorem (First order Karush-Kuhn-Tucker (KKT) Necessary Conditions)** \n",
    "\n",
    "Let $x^*$ be a local minimum for problem\n",
    "$$\n",
    "$$\n",
    "\\begin{align} \\\n",
    "\\min \\quad &f(x)\\\\\n",
    "\\text{s.t.} \\quad & g_j(x) \\geq 0\\text{ for all }j=1,\\ldots,J\\\\\n",
    "& h_k(x) = 0\\text{ for all }k=1,\\ldots,K\\\\\n",
    "&x\\in \\mathbb R^n.\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\n",
    "Let us assume that objective and constraint functions are continuosly differentiable at a point $x^*$ and assume that $x^*$ satisfies some regularity conditions (see e.g., https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions#Regularity_conditions_.28or_constraint_qualifications.29 ). Then there exists unique Lagrance multiplier vectors $\\mu^*=(\\mu_1^*,\\ldots,\\mu_J^*)$ and $\\lambda^* = (\\lambda^*_1,\\ldots,\\lambda_K^*)$ such that\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\nabla_xL(x^*,\\lambda^*,\\mu^*) = 0\\\\\n",
    "&\\mu_j^*\\geq0,\\text{ for all }j=1,\\ldots,J\\\\\n",
    "&\\mu_j^*g_j(x^*)=0,\\text{for all }j=1,\\ldots,J,\n",
    "\\end{align}\n",
    "$$\n",
    "where $L$ is the *Lagrangian function* $$L(x,\\lambda,\\mu) = f(x)- \\sum_{j=1}^J\\mu_jg_j(x) -\\sum_{k=1}^K\\lambda_kh_k(x)$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of constraint qualifications for inequality constraint problems\n",
    "**Definition (regular point)**\n",
    "\n",
    "A point $x^*\\in S$ is *regular* if the set of gradients of the active inequality constraints \n",
    "$$\n",
    "\\{\\nabla g_j(x^*) | \\text{ constraint } i \\text{ is active}\\}\n",
    "$$\n",
    "is linearly independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KKT conditions were developed independently by \n",
    "* Karush:\"Minima of Functions of Several Variables with Inequalities as Side Constraints\". *M.Sc. Dissertation*, Dept. of Mathematics, Univ. of Chicago, 1939\n",
    "* Kuhn & Tucker: \"Nonlinear programming\", In: *Proceedings of 2nd Berkeley Symposium*, pp. 481â€“492, 1951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients $\\lambda$ and $\\mu$ are called the KKT multipliers.\n",
    "\n",
    "The first equality $\\nabla_xL(x,\\lambda,\\mu) = 0$ is called stationary rule and the requirement $\\mu_j^*g_j(x)=0,\\text{for all }j=1,\\ldots,J$ is called the complementary rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Consider the optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\qquad (x_1^2+x^2_2+x^2_3)\\\\\n",
    "\\text{s.t}&\\qquad -3+x_1+x_2+x_3\\geq 0.\n",
    "\\end{align}\n",
    "$$\n",
    "Let us verify the KKT necessary conditions for the local optimum $x^*=(1,1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that\n",
    "$$\n",
    "L(x,\\lambda,\\mu) = (x_1^2+x_2^2+x_3^3)+\\mu_1(3-x_1-x_2-x_3)\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "\\nabla_x L(x,\\lambda,\\mu) = (2x_1-\\mu_1,2x_2-\\mu_1,2x_3-\\mu_1)\n",
    "$$\n",
    "and if $\\nabla_x L((1,1,1),\\lambda,\\mu)=0$, then \n",
    "$$\n",
    "2-\\mu_1=0 $$\n",
    "which holds when $$\n",
    "\\mu_1=2.\n",
    "$$\n",
    "In addition to this, we can see that $-3+x^*_1+x^*_2+x^*_3= 0$. Thus, the completementary rule holds even though $\\mu_1\\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us check the KKT for a solution that is not a local optimum. Let us have $x^*=(0,1,1)$.\n",
    "\n",
    "We can easily see that in this case, the conditions are \n",
    "\n",
    "$$\\left\\{\n",
    "\\begin{array}{c}\n",
    "-\\mu_1 = 0\\\\\n",
    "2-\\mu_1=0\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "Clearly, there does not exist a $\\mu_1\\in \\mathbb R$ such that these equalities would hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us check the KKT for another solution that is not a local optimum. Let us have $x^*=(2,2,2)$.\n",
    "\n",
    "We can easily see that in this case, the conditions is\n",
    "$$\n",
    "4-\\mu_1 = 0\n",
    "$$\n",
    "Now, $\\mu_1=4$ satisfies this equation. However, now\n",
    "$$\n",
    "\\mu_1(-3+x^*_1+x^*_2+x^*_3)=4(-3+6) = 12 \\neq 0.\n",
    "$$\n",
    "Thus, the completementary rule fails and the KKT conditions are not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geometric interpretation of the KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stationary rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stationary rule can be written as, there exist $\\lambda',\\mu$ so that\n",
    "$$\n",
    "-\\nabla f(x) = -\\sum_{j=1}^K\\mu_j\\nabla g_j(x) + \\sum_{k=1}^K\\lambda'_k\\nabla h_k(x).\n",
    "$$\n",
    "Notice that we have slightly different $\\lambda'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, remember that the $-\\nabla v(x)$ gives us the direction of reduction of the function $v$ for all functions.\n",
    "\n",
    "Thus, the above equation means that the direction of reduction of the function $-\\nabla f(x)$ is countered by the direction of the reduction of the inequality constraints $-\\nabla g_j(x)$ and the directions of either growth (or reduction, since $\\lambda'$ can be negative) of the equality constraints $\\nabla h_k(x)$.\n",
    "\n",
    "**This means that the function cannot get reduced without reducing the inequality constraints (making the solution infeasible, if already at the bound), or increasing or decreasing the equality constraints (making, thus, the solution again infeasible).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### With just one inequality constraint this means that the negative gradients of $f$ and $g$ must point to the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![alt text](images/KKT_inequality_constraints.svg \"KKT with inequality constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### With equality constraints this means that the negative of the function and the gradiemt of the equality constraint must either to the same or opposite directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/KKT_equality_constraints.svg \"KKT with inequality constraint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Complementary conditions\n",
    "Another way of saying complementary condition\n",
    "$$\n",
    "\\mu_jg_j(x) = 0 \\text{ for all } j=1,\\ldots,J\n",
    "$$\n",
    "is to say that both $\\mu_j$ and $g_j(x)$ cannot be positive at the same time. Especially, if $\\mu_j>0$, then $g_j(x)=0$.\n",
    "\n",
    "**This means that if we want to use the gradient of a constraint for countering the reduction of the function, then the constraint must be at the border.**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
